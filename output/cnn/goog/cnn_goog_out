                High
timestamp           
2004-08-19  0.002401
2004-08-20  0.007596
2004-08-21  0.007596
2004-08-22  0.007596
2004-08-23  0.012150
2004-08-24  0.010204
2004-08-25  0.006479
2004-08-26  0.006427
2004-08-27  0.007120
2004-08-28  0.007120
                High     y_t+1
timestamp                     
2004-08-19  0.002401  0.007596
2004-08-20  0.007596  0.007596
2004-08-21  0.007596  0.007596
2004-08-22  0.007596  0.012150
2004-08-23  0.012150  0.010204
2004-08-24  0.010204  0.006479
2004-08-25  0.006479  0.006427
2004-08-26  0.006427  0.007120
2004-08-27  0.007120  0.007120
2004-08-28  0.007120  0.007120
            High_original     y_t+1  High_t-9  ...  High_t-2  High_t-1  High_t-0
timestamp                                      ...                              
2004-08-19       0.002401  0.007596       NaN  ...       NaN       NaN  0.002401
2004-08-20       0.007596  0.007596       NaN  ...       NaN  0.002401  0.007596
2004-08-21       0.007596  0.007596       NaN  ...  0.002401  0.007596  0.007596
2004-08-22       0.007596  0.012150       NaN  ...  0.007596  0.007596  0.007596
2004-08-23       0.012150  0.010204       NaN  ...  0.007596  0.007596  0.012150
2004-08-24       0.010204  0.006479       NaN  ...  0.007596  0.012150  0.010204
2004-08-25       0.006479  0.006427       NaN  ...  0.012150  0.010204  0.006479
2004-08-26       0.006427  0.007120       NaN  ...  0.010204  0.006479  0.006427
2004-08-27       0.007120  0.007120       NaN  ...  0.006479  0.006427  0.007120
2004-08-28       0.007120  0.007120  0.002401  ...  0.006427  0.007120  0.007120

[10 rows x 12 columns]
            High_original     y_t+1  High_t-9  ...  High_t-2  High_t-1  High_t-0
timestamp                                      ...                              
2004-08-28       0.007120  0.007120  0.002401  ...  0.006427  0.007120  0.007120
2004-08-29       0.007120  0.003881  0.007596  ...  0.007120  0.007120  0.007120
2004-08-30       0.003881  0.002039  0.007596  ...  0.007120  0.007120  0.003881
2004-08-31       0.002039  0.001273  0.007596  ...  0.007120  0.003881  0.002039
2004-09-01       0.001273  0.000652  0.012150  ...  0.003881  0.002039  0.001273
...                   ...       ...       ...  ...       ...       ...       ...
2013-11-30       0.998572  0.998572  0.969273  ...  1.000000  0.998572  0.998572
2013-12-01       0.998572  0.998292  0.967059  ...  0.998572  0.998572  0.998572
2013-12-02       0.998292  0.995281  0.967059  ...  0.998572  0.998572  0.998292
2013-12-03       0.995281  0.995840  0.967059  ...  0.998572  0.998292  0.995281
2013-12-04       0.995840  0.991369  0.984673  ...  0.998292  0.995281  0.995840

[3386 rows x 12 columns]
(3386, 1)
[[0.00712024]
 [0.00388095]
 [0.00203879]]
(3386, 10, 1)
[[[0.00240101]
  [0.0075963 ]
  [0.0075963 ]
  [0.0075963 ]
  [0.01214994]
  [0.01020429]
  [0.00647859]
  [0.00642684]
  [0.00712024]
  [0.00712024]]

 [[0.0075963 ]
  [0.0075963 ]
  [0.0075963 ]
  [0.01214994]
  [0.01020429]
  [0.00647859]
  [0.00642684]
  [0.00712024]
  [0.00712024]
  [0.00712024]]

 [[0.0075963 ]
  [0.0075963 ]
  [0.01214994]
  [0.01020429]
  [0.00647859]
  [0.00642684]
  [0.00712024]
  [0.00712024]
  [0.00712024]
  [0.00388095]]]
                  High
timestamp             
2013-11-27  532.005920
2013-11-28  532.005920
2013-11-29  531.318481
2013-11-30  531.318481
2013-12-01  531.318481
                High
timestamp           
2013-11-27  1.000000
2013-11-28  1.000000
2013-11-29  0.998572
2013-11-30  0.998572
2013-12-01  0.998572
(1132,)
(1132, 10, 1)
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_1 (Conv1D)            (None, 10, 5)             15        
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 10, 5)             55        
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 10, 5)             55        
_________________________________________________________________
flatten_1 (Flatten)          (None, 50)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 51        
=================================================================
Total params: 176
Trainable params: 176
Non-trainable params: 0
_________________________________________________________________
None
Train on 3386 samples, validate on 1132 samples
Epoch 1/10

  32/3386 [..............................] - ETA: 33s - loss: 0.3520
 768/3386 [=====>........................] - ETA: 1s - loss: 0.1506 
1504/3386 [============>.................] - ETA: 0s - loss: 0.0850
2240/3386 [==================>...........] - ETA: 0s - loss: 0.0590
2976/3386 [=========================>....] - ETA: 0s - loss: 0.0454
3386/3386 [==============================] - 1s 189us/step - loss: 0.0404 - val_loss: 0.0571
Epoch 2/10

  32/3386 [..............................] - ETA: 0s - loss: 0.0032
 800/3386 [======>.......................] - ETA: 0s - loss: 0.0028
1472/3386 [============>.................] - ETA: 0s - loss: 0.0028
2272/3386 [===================>..........] - ETA: 0s - loss: 0.0025
3136/3386 [==========================>...] - ETA: 0s - loss: 0.0023
3386/3386 [==============================] - 0s 76us/step - loss: 0.0022 - val_loss: 0.0191
Epoch 3/10

  32/3386 [..............................] - ETA: 0s - loss: 7.9915e-04
 896/3386 [======>.......................] - ETA: 0s - loss: 9.6809e-04
1728/3386 [==============>...............] - ETA: 0s - loss: 9.0597e-04
2528/3386 [=====================>........] - ETA: 0s - loss: 8.1230e-04
3386/3386 [==============================] - 0s 72us/step - loss: 7.4018e-04 - val_loss: 0.0049
Epoch 4/10

  32/3386 [..............................] - ETA: 0s - loss: 4.5133e-04
 864/3386 [======>.......................] - ETA: 0s - loss: 3.9133e-04
1696/3386 [==============>...............] - ETA: 0s - loss: 4.2498e-04
2496/3386 [=====================>........] - ETA: 0s - loss: 4.3135e-04
3264/3386 [===========================>..] - ETA: 0s - loss: 4.2360e-04
3386/3386 [==============================] - 0s 74us/step - loss: 4.2419e-04 - val_loss: 0.0024
Epoch 5/10

  32/3386 [..............................] - ETA: 0s - loss: 4.9394e-04
 832/3386 [======>.......................] - ETA: 0s - loss: 3.8618e-04
1696/3386 [==============>...............] - ETA: 0s - loss: 3.8032e-04
2528/3386 [=====================>........] - ETA: 0s - loss: 3.8278e-04
3328/3386 [============================>.] - ETA: 0s - loss: 3.8352e-04
3386/3386 [==============================] - 0s 74us/step - loss: 3.8365e-04 - val_loss: 0.0022
Epoch 6/10

  32/3386 [..............................] - ETA: 0s - loss: 6.1605e-04
 864/3386 [======>.......................] - ETA: 0s - loss: 3.7852e-04
1696/3386 [==============>...............] - ETA: 0s - loss: 3.7421e-04
2592/3386 [=====================>........] - ETA: 0s - loss: 3.6946e-04
3386/3386 [==============================] - 0s 68us/step - loss: 3.6931e-04 - val_loss: 0.0020
Epoch 7/10

  32/3386 [..............................] - ETA: 0s - loss: 4.4948e-04
 896/3386 [======>.......................] - ETA: 0s - loss: 3.3967e-04
1760/3386 [==============>...............] - ETA: 0s - loss: 3.7787e-04
2624/3386 [======================>.......] - ETA: 0s - loss: 3.7099e-04
3386/3386 [==============================] - 0s 69us/step - loss: 3.6057e-04 - val_loss: 0.0018
Epoch 8/10

  32/3386 [..............................] - ETA: 0s - loss: 2.8196e-04
 896/3386 [======>.......................] - ETA: 0s - loss: 3.6658e-04
1760/3386 [==============>...............] - ETA: 0s - loss: 3.5382e-04
2624/3386 [======================>.......] - ETA: 0s - loss: 3.4066e-04
3386/3386 [==============================] - 0s 68us/step - loss: 3.5189e-04 - val_loss: 0.0014
Epoch 9/10

  32/3386 [..............................] - ETA: 0s - loss: 2.9528e-04
 960/3386 [=======>......................] - ETA: 0s - loss: 3.3513e-04
1792/3386 [==============>...............] - ETA: 0s - loss: 3.2762e-04
2560/3386 [=====================>........] - ETA: 0s - loss: 3.3884e-04
3360/3386 [============================>.] - ETA: 0s - loss: 3.4622e-04
3386/3386 [==============================] - 0s 71us/step - loss: 3.4550e-04 - val_loss: 0.0014
Epoch 10/10

  32/3386 [..............................] - ETA: 0s - loss: 4.6514e-04
 928/3386 [=======>......................] - ETA: 0s - loss: 3.2746e-04
1824/3386 [===============>..............] - ETA: 0s - loss: 3.3420e-04
2688/3386 [======================>.......] - ETA: 0s - loss: 3.3530e-04
3386/3386 [==============================] - 0s 70us/step - loss: 3.4086e-04 - val_loss: 0.0016
                  High
timestamp             
2017-01-12  807.390015
2017-01-13  811.223999
2017-01-14  811.223999
2017-01-15  811.223999
2017-01-16  811.223999
                High
timestamp           
2017-01-12  1.572136
2017-01-13  1.580102
2017-01-14  1.580102
2017-01-15  1.580102
2017-01-16  1.580102
[[1.5477065]
 [1.5493113]
 [1.5514456]
 ...
 [2.9062474]
 [2.9040077]
 [2.921712 ]]
   timestamp    h  prediction      actual
0 2017-01-21  t+1  795.631227  806.909973
1 2017-01-22  t+1  796.403657  820.869995
2 2017-01-23  t+1  797.430962  825.900024
3 2017-01-24  t+1  795.807838  835.770020
4 2017-01-25  t+1  796.728534  838.000000
rmse:  50.40641361678926  mse:  2540.806533706838 evs:  0.9615967565396388 mae:  43.0236130640093 msle:  0.0019488828824355043 meae:  40.39186431715473 r_square:  0.8904613753498838
