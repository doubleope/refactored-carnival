                High
timestamp           
2004-08-19  0.038047
2004-08-20  0.036517
2004-08-21  0.036517
2004-08-22  0.036517
2004-08-23  0.036759
2004-08-24  0.036571
2004-08-25  0.038074
2004-08-26  0.038986
2004-08-27  0.037805
2004-08-28  0.037805
                High     y_t+1
timestamp                     
2004-08-19  0.038047  0.036517
2004-08-20  0.036517  0.036517
2004-08-21  0.036517  0.036517
2004-08-22  0.036517  0.036759
2004-08-23  0.036759  0.036571
2004-08-24  0.036571  0.038074
2004-08-25  0.038074  0.038986
2004-08-26  0.038986  0.037805
2004-08-27  0.037805  0.037805
2004-08-28  0.037805  0.037805
            High_original     y_t+1  High_t-9  ...  High_t-2  High_t-1  High_t-0
timestamp                                      ...                              
2004-08-19       0.038047  0.036517       NaN  ...       NaN       NaN  0.038047
2004-08-20       0.036517  0.036517       NaN  ...       NaN  0.038047  0.036517
2004-08-21       0.036517  0.036517       NaN  ...  0.038047  0.036517  0.036517
2004-08-22       0.036517  0.036759       NaN  ...  0.036517  0.036517  0.036517
2004-08-23       0.036759  0.036571       NaN  ...  0.036517  0.036517  0.036759
2004-08-24       0.036571  0.038074       NaN  ...  0.036517  0.036759  0.036571
2004-08-25       0.038074  0.038986       NaN  ...  0.036759  0.036571  0.038074
2004-08-26       0.038986  0.037805       NaN  ...  0.036571  0.038074  0.038986
2004-08-27       0.037805  0.037805       NaN  ...  0.038074  0.038986  0.037805
2004-08-28       0.037805  0.037805  0.038047  ...  0.038986  0.037805  0.037805

[10 rows x 12 columns]
            High_original     y_t+1  High_t-9  ...  High_t-2  High_t-1  High_t-0
timestamp                                      ...                              
2004-08-28       0.037805  0.037805  0.038047  ...  0.038986  0.037805  0.037805
2004-08-29       0.037805  0.036625  0.036517  ...  0.037805  0.037805  0.037805
2004-08-30       0.036625  0.033673  0.036517  ...  0.037805  0.037805  0.036625
2004-08-31       0.033673  0.034102  0.036517  ...  0.037805  0.036625  0.033673
2004-09-01       0.034102  0.035793  0.036759  ...  0.036625  0.033673  0.034102
...                   ...       ...       ...  ...       ...       ...       ...
2013-11-30       0.986853  0.986853  0.920177  ...  0.967803  0.986853  0.986853
2013-12-01       0.986853  1.000000  0.934263  ...  0.986853  0.986853  0.986853
2013-12-02       1.000000  0.978401  0.934263  ...  0.986853  0.986853  1.000000
2013-12-03       0.978401  0.975020  0.934263  ...  0.986853  1.000000  0.978401
2013-12-04       0.975020  0.966863  0.943091  ...  1.000000  0.978401  0.975020

[3386 rows x 12 columns]
(3386, 1)
[[0.03780521]
 [0.03662464]
 [0.03367319]]
(3386, 10, 1)
[[[0.03804669]
  [0.03651731]
  [0.03651731]
  [0.03651731]
  [0.03675879]
  [0.03657097]
  [0.03807352]
  [0.03898579]
  [0.03780521]
  [0.03780521]]

 [[0.03651731]
  [0.03651731]
  [0.03651731]
  [0.03675879]
  [0.03657097]
  [0.03807352]
  [0.03898579]
  [0.03780521]
  [0.03780521]
  [0.03780521]]

 [[0.03651731]
  [0.03651731]
  [0.03675879]
  [0.03657097]
  [0.03807352]
  [0.03898579]
  [0.03780521]
  [0.03780521]
  [0.03780521]
  [0.03662464]]]
                  High
timestamp             
2013-11-27  387.000000
2013-11-28  387.000000
2013-11-29  394.100006
2013-11-30  394.100006
2013-12-01  394.100006
                High
timestamp           
2013-11-27  0.967803
2013-11-28  0.967803
2013-11-29  0.986853
2013-11-30  0.986853
2013-12-01  0.986853
(1132,)
(1132, 10, 1)
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_1 (Conv1D)            (None, 10, 5)             15        
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 10, 5)             55        
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 10, 5)             55        
_________________________________________________________________
flatten_1 (Flatten)          (None, 50)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 51        
=================================================================
Total params: 176
Trainable params: 176
Non-trainable params: 0
_________________________________________________________________
None
Train on 3386 samples, validate on 1132 samples
Epoch 1/10

  32/3386 [..............................] - ETA: 24s - loss: 0.2796
 800/3386 [======>.......................] - ETA: 0s - loss: 0.2341 
1568/3386 [============>.................] - ETA: 0s - loss: 0.1779
2368/3386 [===================>..........] - ETA: 0s - loss: 0.1460
3136/3386 [==========================>...] - ETA: 0s - loss: 0.1261
3386/3386 [==============================] - 1s 155us/step - loss: 0.1199 - val_loss: 1.4179
Epoch 2/10

  32/3386 [..............................] - ETA: 0s - loss: 0.0495
 800/3386 [======>.......................] - ETA: 0s - loss: 0.0383
1632/3386 [=============>................] - ETA: 0s - loss: 0.0338
2400/3386 [====================>.........] - ETA: 0s - loss: 0.0292
3232/3386 [===========================>..] - ETA: 0s - loss: 0.0239
3386/3386 [==============================] - 0s 73us/step - loss: 0.0230 - val_loss: 0.5263
Epoch 3/10

  32/3386 [..............................] - ETA: 0s - loss: 0.0014
 896/3386 [======>.......................] - ETA: 0s - loss: 0.0017
1760/3386 [==============>...............] - ETA: 0s - loss: 0.0015
2592/3386 [=====================>........] - ETA: 0s - loss: 0.0013
3386/3386 [==============================] - 0s 70us/step - loss: 0.0011 - val_loss: 0.2997
Epoch 4/10

  32/3386 [..............................] - ETA: 0s - loss: 0.0012
 864/3386 [======>.......................] - ETA: 0s - loss: 6.2525e-04
1664/3386 [=============>................] - ETA: 0s - loss: 6.7979e-04
2528/3386 [=====================>........] - ETA: 0s - loss: 6.3296e-04
3360/3386 [============================>.] - ETA: 0s - loss: 5.8889e-04
3386/3386 [==============================] - 0s 72us/step - loss: 5.8920e-04 - val_loss: 0.2436
Epoch 5/10

  32/3386 [..............................] - ETA: 0s - loss: 3.2669e-04
 864/3386 [======>.......................] - ETA: 0s - loss: 4.7158e-04
1728/3386 [==============>...............] - ETA: 0s - loss: 4.6830e-04
2592/3386 [=====================>........] - ETA: 0s - loss: 4.4692e-04
3386/3386 [==============================] - 0s 71us/step - loss: 4.5710e-04 - val_loss: 0.2093
Epoch 6/10

  32/3386 [..............................] - ETA: 0s - loss: 0.0011
 928/3386 [=======>......................] - ETA: 0s - loss: 4.6182e-04
1760/3386 [==============>...............] - ETA: 0s - loss: 4.2904e-04
2560/3386 [=====================>........] - ETA: 0s - loss: 3.9421e-04
3386/3386 [==============================] - 0s 70us/step - loss: 3.8285e-04 - val_loss: 0.1842
Epoch 7/10

  32/3386 [..............................] - ETA: 0s - loss: 3.1012e-04
 896/3386 [======>.......................] - ETA: 0s - loss: 3.5581e-04
1728/3386 [==============>...............] - ETA: 0s - loss: 3.6239e-04
2592/3386 [=====================>........] - ETA: 0s - loss: 3.4225e-04
3386/3386 [==============================] - 0s 72us/step - loss: 3.4482e-04 - val_loss: 0.1670
Epoch 8/10

  32/3386 [..............................] - ETA: 0s - loss: 3.4260e-04
 800/3386 [======>.......................] - ETA: 0s - loss: 3.2477e-04
1536/3386 [============>.................] - ETA: 0s - loss: 3.2506e-04
2400/3386 [====================>.........] - ETA: 0s - loss: 3.0780e-04
3168/3386 [===========================>..] - ETA: 0s - loss: 3.1908e-04
3386/3386 [==============================] - 0s 74us/step - loss: 3.1765e-04 - val_loss: 0.1567
Epoch 9/10

  32/3386 [..............................] - ETA: 0s - loss: 4.3643e-04
 864/3386 [======>.......................] - ETA: 0s - loss: 2.9063e-04
1696/3386 [==============>...............] - ETA: 0s - loss: 2.9324e-04
2528/3386 [=====================>........] - ETA: 0s - loss: 3.0195e-04
3360/3386 [============================>.] - ETA: 0s - loss: 2.9325e-04
3386/3386 [==============================] - 0s 70us/step - loss: 2.9270e-04 - val_loss: 0.1482
Epoch 10/10

  32/3386 [..............................] - ETA: 0s - loss: 2.6361e-04
 928/3386 [=======>......................] - ETA: 0s - loss: 2.3007e-04
1792/3386 [==============>...............] - ETA: 0s - loss: 2.4132e-04
2624/3386 [======================>.......] - ETA: 0s - loss: 2.5705e-04
3386/3386 [==============================] - 0s 70us/step - loss: 2.6988e-04 - val_loss: 0.1405
                  High
timestamp             
2017-01-12  814.130005
2017-01-13  821.650024
2017-01-14  821.650024
2017-01-15  821.650024
2017-01-16  821.650024
                High
timestamp           
2017-01-12  2.113845
2017-01-13  2.134022
2017-01-14  2.134022
2017-01-15  2.134022
2017-01-16  2.134022
[[1.3402566]
 [1.3483163]
 [1.350271 ]
 ...
 [2.2393987]
 [2.2704966]
 [2.3162448]]
   timestamp    h  prediction      actual
0 2017-01-21  t+1  525.813625  816.020020
1 2017-01-22  t+1  528.817490  818.500000
2 2017-01-23  t+1  529.545997  823.989990
3 2017-01-24  t+1  529.086909  837.419983
4 2017-01-25  t+1  528.423357  843.840027
rmse:  853.977338241754  mse:  729277.294230471 evs:  0.4502950015965703 mae:  804.4033649409522 msle:  0.5519053585447768 meae:  902.3410620880402 r_square:  -3.8762326034623333
