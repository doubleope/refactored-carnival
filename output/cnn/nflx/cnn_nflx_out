                High
timestamp           
2004-08-19  0.018240
2004-08-20  0.017845
2004-08-21  0.017845
2004-08-22  0.017845
2004-08-23  0.018503
2004-08-24  0.018161
2004-08-25  0.017713
2004-08-26  0.017029
2004-08-27  0.016187
2004-08-28  0.016187
                High     y_t+1
timestamp                     
2004-08-19  0.018240  0.017845
2004-08-20  0.017845  0.017845
2004-08-21  0.017845  0.017845
2004-08-22  0.017845  0.018503
2004-08-23  0.018503  0.018161
2004-08-24  0.018161  0.017713
2004-08-25  0.017713  0.017029
2004-08-26  0.017029  0.016187
2004-08-27  0.016187  0.016187
2004-08-28  0.016187  0.016187
            High_original     y_t+1  High_t-9  ...  High_t-2  High_t-1  High_t-0
timestamp                                      ...                              
2004-08-19       0.018240  0.017845       NaN  ...       NaN       NaN  0.018240
2004-08-20       0.017845  0.017845       NaN  ...       NaN  0.018240  0.017845
2004-08-21       0.017845  0.017845       NaN  ...  0.018240  0.017845  0.017845
2004-08-22       0.017845  0.018503       NaN  ...  0.017845  0.017845  0.017845
2004-08-23       0.018503  0.018161       NaN  ...  0.017845  0.017845  0.018503
2004-08-24       0.018161  0.017713       NaN  ...  0.017845  0.018503  0.018161
2004-08-25       0.017713  0.017029       NaN  ...  0.018503  0.018161  0.017713
2004-08-26       0.017029  0.016187       NaN  ...  0.018161  0.017713  0.017029
2004-08-27       0.016187  0.016187       NaN  ...  0.017713  0.017029  0.016187
2004-08-28       0.016187  0.016187   0.01824  ...  0.017029  0.016187  0.016187

[10 rows x 12 columns]
            High_original     y_t+1  High_t-9  ...  High_t-2  High_t-1  High_t-0
timestamp                                      ...                              
2004-08-28       0.016187  0.016187  0.018240  ...  0.017029  0.016187  0.016187
2004-08-29       0.016187  0.015476  0.017845  ...  0.016187  0.016187  0.016187
2004-08-30       0.015476  0.014160  0.017845  ...  0.016187  0.016187  0.015476
2004-08-31       0.014160  0.013950  0.017845  ...  0.016187  0.015476  0.014160
2004-09-01       0.013950  0.015476  0.018503  ...  0.015476  0.014160  0.013950
...                   ...       ...       ...  ...       ...       ...       ...
2013-11-30       0.943781  0.943781  0.893457  ...  0.933384  0.943781  0.943781
2013-12-01       0.943781  0.944202  0.898510  ...  0.943781  0.943781  0.943781
2013-12-02       0.944202  0.932674  0.898510  ...  0.943781  0.943781  0.944202
2013-12-03       0.932674  0.946018  0.898510  ...  0.943781  0.944202  0.932674
2013-12-04       0.946018  0.926225  0.904643  ...  0.944202  0.932674  0.946018

[3386 rows x 12 columns]
(3386, 1)
[[0.01618677]
 [0.01547613]
 [0.01416013]]
(3386, 10, 1)
[[[0.01823972]
  [0.01784493]
  [0.01784493]
  [0.01784493]
  [0.01850292]
  [0.01816076]
  [0.01771332]
  [0.01702901]
  [0.01618677]
  [0.01618677]]

 [[0.01784493]
  [0.01784493]
  [0.01784493]
  [0.01850292]
  [0.01816076]
  [0.01771332]
  [0.01702901]
  [0.01618677]
  [0.01618677]
  [0.01618677]]

 [[0.01784493]
  [0.01784493]
  [0.01850292]
  [0.01816076]
  [0.01771332]
  [0.01702901]
  [0.01618677]
  [0.01618677]
  [0.01618677]
  [0.01547613]]]
                 High
timestamp            
2013-11-27  51.978573
2013-11-28  51.978573
2013-11-29  52.542858
2013-11-30  52.542858
2013-12-01  52.542858
                High
timestamp           
2013-11-27  0.933384
2013-11-28  0.933384
2013-11-29  0.943781
2013-11-30  0.943781
2013-12-01  0.943781
(1132,)
(1132, 10, 1)
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_1 (Conv1D)            (None, 10, 5)             15        
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 10, 5)             55        
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 10, 5)             55        
_________________________________________________________________
flatten_1 (Flatten)          (None, 50)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 51        
=================================================================
Total params: 176
Trainable params: 176
Non-trainable params: 0
_________________________________________________________________
None
Train on 3386 samples, validate on 1132 samples
Epoch 1/10

  32/3386 [..............................] - ETA: 25s - loss: 0.0842
 800/3386 [======>.......................] - ETA: 0s - loss: 0.0684 
1568/3386 [============>.................] - ETA: 0s - loss: 0.0560
2336/3386 [===================>..........] - ETA: 0s - loss: 0.0458
3072/3386 [==========================>...] - ETA: 0s - loss: 0.0375
3386/3386 [==============================] - 1s 161us/step - loss: 0.0346 - val_loss: 0.4669
Epoch 2/10

  32/3386 [..............................] - ETA: 0s - loss: 0.0022
 800/3386 [======>.......................] - ETA: 0s - loss: 0.0023
1600/3386 [=============>................] - ETA: 0s - loss: 0.0017
2400/3386 [====================>.........] - ETA: 0s - loss: 0.0014
3232/3386 [===========================>..] - ETA: 0s - loss: 0.0013
3386/3386 [==============================] - 0s 73us/step - loss: 0.0013 - val_loss: 0.1456
Epoch 3/10

  32/3386 [..............................] - ETA: 0s - loss: 0.0012
 864/3386 [======>.......................] - ETA: 0s - loss: 9.0081e-04
1696/3386 [==============>...............] - ETA: 0s - loss: 8.2678e-04
2496/3386 [=====================>........] - ETA: 0s - loss: 8.3063e-04
3328/3386 [============================>.] - ETA: 0s - loss: 8.2991e-04
3386/3386 [==============================] - 0s 73us/step - loss: 8.4524e-04 - val_loss: 0.1169
Epoch 4/10

  32/3386 [..............................] - ETA: 0s - loss: 0.0010
 832/3386 [======>.......................] - ETA: 0s - loss: 7.5353e-04
1600/3386 [=============>................] - ETA: 0s - loss: 7.8579e-04
2400/3386 [====================>.........] - ETA: 0s - loss: 7.7392e-04
3200/3386 [===========================>..] - ETA: 0s - loss: 8.0196e-04
3386/3386 [==============================] - 0s 74us/step - loss: 7.9562e-04 - val_loss: 0.1061
Epoch 5/10

  32/3386 [..............................] - ETA: 0s - loss: 6.9836e-04
 832/3386 [======>.......................] - ETA: 0s - loss: 7.2881e-04
1696/3386 [==============>...............] - ETA: 0s - loss: 7.9925e-04
2528/3386 [=====================>........] - ETA: 0s - loss: 7.7089e-04
3386/3386 [==============================] - 0s 71us/step - loss: 7.6495e-04 - val_loss: 0.0994
Epoch 6/10

  32/3386 [..............................] - ETA: 0s - loss: 0.0024
 864/3386 [======>.......................] - ETA: 0s - loss: 7.1723e-04
1696/3386 [==============>...............] - ETA: 0s - loss: 7.1119e-04
2528/3386 [=====================>........] - ETA: 0s - loss: 6.6317e-04
3360/3386 [============================>.] - ETA: 0s - loss: 7.3434e-04
3386/3386 [==============================] - 0s 72us/step - loss: 7.3109e-04 - val_loss: 0.1067
Epoch 7/10

  32/3386 [..............................] - ETA: 0s - loss: 4.8574e-04
 864/3386 [======>.......................] - ETA: 0s - loss: 6.4110e-04
1696/3386 [==============>...............] - ETA: 0s - loss: 6.8464e-04
2528/3386 [=====================>........] - ETA: 0s - loss: 7.4222e-04
3386/3386 [==============================] - 0s 71us/step - loss: 7.1668e-04 - val_loss: 0.1031
Epoch 8/10

  32/3386 [..............................] - ETA: 0s - loss: 1.1176e-04
 896/3386 [======>.......................] - ETA: 0s - loss: 5.6613e-04
1728/3386 [==============>...............] - ETA: 0s - loss: 6.1104e-04
2560/3386 [=====================>........] - ETA: 0s - loss: 6.9239e-04
3386/3386 [==============================] - 0s 72us/step - loss: 6.8730e-04 - val_loss: 0.1054
Epoch 9/10

  32/3386 [..............................] - ETA: 0s - loss: 2.9233e-04
 864/3386 [======>.......................] - ETA: 0s - loss: 6.4716e-04
1600/3386 [=============>................] - ETA: 0s - loss: 6.6960e-04
2496/3386 [=====================>........] - ETA: 0s - loss: 6.6912e-04
3386/3386 [==============================] - 0s 70us/step - loss: 6.5562e-04 - val_loss: 0.0982
Epoch 10/10

  32/3386 [..............................] - ETA: 0s - loss: 9.0001e-04
 896/3386 [======>.......................] - ETA: 0s - loss: 7.3163e-04
1824/3386 [===============>..............] - ETA: 0s - loss: 6.1094e-04
2688/3386 [======================>.......] - ETA: 0s - loss: 6.0849e-04
3386/3386 [==============================] - 0s 66us/step - loss: 6.3647e-04 - val_loss: 0.1100
                  High
timestamp             
2017-01-12  130.850006
2017-01-13  133.929993
2017-01-14  133.929993
2017-01-15  133.929993
2017-01-16  133.929993
                High
timestamp           
2017-01-12  2.386509
2017-01-13  2.443254
2017-01-14  2.443254
2017-01-15  2.443254
2017-01-16  2.443254
[[1.8366092]
 [1.8716203]
 [1.8544921]
 ...
 [4.519778 ]
 [4.559749 ]
 [4.5693717]]
   timestamp    h  prediction      actual
0 2017-01-21  t+1  101.003042  140.789993
1 2017-01-22  t+1  102.903342  139.490005
2 2017-01-23  t+1  101.973671  140.929993
3 2017-01-24  t+1  102.170563  141.389999
4 2017-01-25  t+1  104.150125  141.210007
rmse:  99.01321477704286  mse:  9803.61670048482 evs:  0.8170249716456275 mae:  92.3932131376076 msle:  0.15241058270391086 meae:  99.3342169995467 r_square:  -0.41567489557500226
