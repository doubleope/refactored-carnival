                High
timestamp           
2004-08-19  0.000631
2004-08-20  0.000000
2004-08-21  0.000000
2004-08-22  0.000000
2004-08-23  0.000203
2004-08-24  0.000696
2004-08-25  0.001566
2004-08-26  0.003038
2004-08-27  0.002734
2004-08-28  0.002734
                High     y_t+1
timestamp                     
2004-08-19  0.000631  0.000000
2004-08-20  0.000000  0.000000
2004-08-21  0.000000  0.000000
2004-08-22  0.000000  0.000203
2004-08-23  0.000203  0.000696
2004-08-24  0.000696  0.001566
2004-08-25  0.001566  0.003038
2004-08-26  0.003038  0.002734
2004-08-27  0.002734  0.002734
2004-08-28  0.002734  0.002734
            High_original     y_t+1  High_t-9  ...  High_t-2  High_t-1  High_t-0
timestamp                                      ...                              
2004-08-19       0.000631  0.000000       NaN  ...       NaN       NaN  0.000631
2004-08-20       0.000000  0.000000       NaN  ...       NaN  0.000631  0.000000
2004-08-21       0.000000  0.000000       NaN  ...  0.000631  0.000000  0.000000
2004-08-22       0.000000  0.000203       NaN  ...  0.000000  0.000000  0.000000
2004-08-23       0.000203  0.000696       NaN  ...  0.000000  0.000000  0.000203
2004-08-24       0.000696  0.001566       NaN  ...  0.000000  0.000203  0.000696
2004-08-25       0.001566  0.003038       NaN  ...  0.000203  0.000696  0.001566
2004-08-26       0.003038  0.002734       NaN  ...  0.000696  0.001566  0.003038
2004-08-27       0.002734  0.002734       NaN  ...  0.001566  0.003038  0.002734
2004-08-28       0.002734  0.002734  0.000631  ...  0.003038  0.002734  0.002734

[10 rows x 12 columns]
            High_original     y_t+1  High_t-9  ...  High_t-2  High_t-1  High_t-0
timestamp                                      ...                              
2004-08-28       0.002734  0.002734  0.000631  ...  0.003038  0.002734  0.002734
2004-08-29       0.002734  0.002705  0.000000  ...  0.002734  0.002734  0.002734
2004-08-30       0.002705  0.002871  0.000000  ...  0.002734  0.002734  0.002705
2004-08-31       0.002871  0.003625  0.000000  ...  0.002734  0.002705  0.002871
2004-09-01       0.003625  0.003495  0.000203  ...  0.002705  0.002871  0.003625
...                   ...       ...       ...  ...       ...       ...       ...
2013-11-30       0.787202  0.787202  0.733372  ...  0.769322  0.787202  0.787202
2013-12-01       0.787202  0.795903  0.734750  ...  0.787202  0.787202  0.787202
2013-12-02       0.795903  0.798876  0.734750  ...  0.787202  0.787202  0.795903
2013-12-03       0.798876  0.802951  0.734750  ...  0.787202  0.795903  0.798876
2013-12-04       0.802951  0.811580  0.740130  ...  0.795903  0.798876  0.802951

[3386 rows x 12 columns]
(3386, 1)
[[0.00273357]
 [0.00270457]
 [0.00287133]]
(3386, 10, 1)
[[[0.00063083]
  [0.        ]
  [0.        ]
  [0.        ]
  [0.00020303]
  [0.00069608]
  [0.00156618]
  [0.0030381 ]
  [0.00273357]
  [0.00273357]]

 [[0.        ]
  [0.        ]
  [0.        ]
  [0.00020303]
  [0.00069608]
  [0.00156618]
  [0.0030381 ]
  [0.00273357]
  [0.00273357]
  [0.00273357]]

 [[0.        ]
  [0.        ]
  [0.00020303]
  [0.00069608]
  [0.00156618]
  [0.0030381 ]
  [0.00273357]
  [0.00273357]
  [0.00273357]
  [0.00270457]]]
                 High
timestamp            
2013-11-27  78.000000
2013-11-28  78.000000
2013-11-29  79.761429
2013-11-30  79.761429
2013-12-01  79.761429
                High
timestamp           
2013-11-27  0.769322
2013-11-28  0.769322
2013-11-29  0.787202
2013-11-30  0.787202
2013-12-01  0.787202
(1132,)
(1132, 10, 1)
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_1 (Conv1D)            (None, 10, 5)             15        
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 10, 5)             55        
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 10, 5)             55        
_________________________________________________________________
flatten_1 (Flatten)          (None, 50)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 51        
=================================================================
Total params: 176
Trainable params: 176
Non-trainable params: 0
_________________________________________________________________
None
Train on 3386 samples, validate on 1132 samples
Epoch 1/10

  32/3386 [..............................] - ETA: 27s - loss: 0.1444
 768/3386 [=====>........................] - ETA: 1s - loss: 0.1635 
1536/3386 [============>.................] - ETA: 0s - loss: 0.1323
2272/3386 [===================>..........] - ETA: 0s - loss: 0.1043
3040/3386 [=========================>....] - ETA: 0s - loss: 0.0812
3386/3386 [==============================] - 1s 167us/step - loss: 0.0732 - val_loss: 0.0955
Epoch 2/10

  32/3386 [..............................] - ETA: 0s - loss: 0.0022
 896/3386 [======>.......................] - ETA: 0s - loss: 0.0013
1760/3386 [==============>...............] - ETA: 0s - loss: 0.0011
2624/3386 [======================>.......] - ETA: 0s - loss: 9.4473e-04
3386/3386 [==============================] - 0s 69us/step - loss: 8.3992e-04 - val_loss: 0.0270
Epoch 3/10

  32/3386 [..............................] - ETA: 0s - loss: 4.2984e-04
 864/3386 [======>.......................] - ETA: 0s - loss: 4.0578e-04
1760/3386 [==============>...............] - ETA: 0s - loss: 4.2975e-04
2656/3386 [======================>.......] - ETA: 0s - loss: 4.1906e-04
3386/3386 [==============================] - 0s 69us/step - loss: 3.9999e-04 - val_loss: 0.0200
Epoch 4/10

  32/3386 [..............................] - ETA: 0s - loss: 1.4773e-04
 896/3386 [======>.......................] - ETA: 0s - loss: 3.4997e-04
1728/3386 [==============>...............] - ETA: 0s - loss: 3.4976e-04
2560/3386 [=====================>........] - ETA: 0s - loss: 3.5277e-04
3386/3386 [==============================] - 0s 69us/step - loss: 3.4781e-04 - val_loss: 0.0173
Epoch 5/10

  32/3386 [..............................] - ETA: 0s - loss: 4.6612e-04
 864/3386 [======>.......................] - ETA: 0s - loss: 3.2444e-04
1728/3386 [==============>...............] - ETA: 0s - loss: 3.4237e-04
2656/3386 [======================>.......] - ETA: 0s - loss: 3.3713e-04
3386/3386 [==============================] - 0s 67us/step - loss: 3.3672e-04 - val_loss: 0.0161
Epoch 6/10

  32/3386 [..............................] - ETA: 0s - loss: 2.6513e-04
 928/3386 [=======>......................] - ETA: 0s - loss: 3.2566e-04
1792/3386 [==============>...............] - ETA: 0s - loss: 3.2381e-04
2656/3386 [======================>.......] - ETA: 0s - loss: 3.2052e-04
3386/3386 [==============================] - 0s 69us/step - loss: 3.2288e-04 - val_loss: 0.0164
Epoch 7/10

  32/3386 [..............................] - ETA: 0s - loss: 1.4132e-04
 896/3386 [======>.......................] - ETA: 0s - loss: 3.1323e-04
1760/3386 [==============>...............] - ETA: 0s - loss: 3.2772e-04
2656/3386 [======================>.......] - ETA: 0s - loss: 3.4085e-04
3386/3386 [==============================] - 0s 70us/step - loss: 3.3693e-04 - val_loss: 0.0162
Epoch 8/10

  32/3386 [..............................] - ETA: 0s - loss: 3.2588e-04
 800/3386 [======>.......................] - ETA: 0s - loss: 3.4663e-04
1696/3386 [==============>...............] - ETA: 0s - loss: 3.4921e-04
2624/3386 [======================>.......] - ETA: 0s - loss: 3.4106e-04
3386/3386 [==============================] - 0s 69us/step - loss: 3.2948e-04 - val_loss: 0.0160
Epoch 9/10

  32/3386 [..............................] - ETA: 0s - loss: 1.0416e-04
 896/3386 [======>.......................] - ETA: 0s - loss: 3.5863e-04
1760/3386 [==============>...............] - ETA: 0s - loss: 3.5551e-04
2624/3386 [======================>.......] - ETA: 0s - loss: 3.3168e-04
3386/3386 [==============================] - 0s 70us/step - loss: 3.2125e-04 - val_loss: 0.0159
Epoch 10/10

  32/3386 [..............................] - ETA: 0s - loss: 3.2072e-04
 992/3386 [=======>......................] - ETA: 0s - loss: 3.5108e-04
1984/3386 [================>.............] - ETA: 0s - loss: 3.6110e-04
2912/3386 [========================>.....] - ETA: 0s - loss: 3.3845e-04
3386/3386 [==============================] - 0s 64us/step - loss: 3.2918e-04 - val_loss: 0.0161
                  High
timestamp             
2017-01-12  119.300003
2017-01-13  119.620003
2017-01-14  119.620003
2017-01-15  119.620003
2017-01-16  119.620003
                High
timestamp           
2017-01-12  1.188565
2017-01-13  1.191814
2017-01-14  1.191814
2017-01-15  1.191814
2017-01-16  1.191814
[[1.021318 ]
 [1.0225084]
 [1.0225084]
 ...
 [1.6066389]
 [1.599041 ]
 [1.6047573]]
   timestamp    h  prediction      actual
0 2017-01-21  t+1  102.824336  120.449997
1 2017-01-22  t+1  102.941606  120.809998
2 2017-01-23  t+1  102.941606  120.099998
3 2017-01-24  t+1  102.947266  122.099998
4 2017-01-25  t+1  102.899494  122.440002
rmse:  75.39166621840154  mse:  5683.9033351868675 evs:  0.46147349288323236 mae:  68.3622070059165 msle:  0.19159197534935382 meae:  60.100664226838475 r_square:  -2.029090841891801
